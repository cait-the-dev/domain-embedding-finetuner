
# Domainâ€‘Specific Embedding Fineâ€‘Tuner ðŸ›¡ï¸

*A laptopâ€‘friendly pipeline that adapts a 1â€¯Bâ€‘parameter Llamaâ€‘3 model to **one** PDF and proves measurable retrieval & QA uplift.*

---

## 1Â Â·Â What this repo does

> â€œPoint at a PDF â†’ get a bespoke GGUF LLM + eval report in **one command**.â€

* **No GPU required** (4â€‘bit QLoRA fits in CPU RAM).  
* **â‰¤â€¯500 synthetic Qâ€‘A** generated with GPTâ€‘4oâ€‘mini.  
* **Hitsâ€¯@â€¯3** retrieval and **LLMâ€‘judge** answer quality both improve ~45â€¯pp.

---

## 2Â Â·Â Architecture

```mermaid
flowchart TD
    A[make all] --> B(ingest.py)
    B --> C(generate.py)
    C --> D(finetune.py)
    D --> E(evaluate.py)
    E -->|optional| F(demo.py)
    subgraph Artifacts
      B -->|jsonl| X[data/pdf_chunks.jsonl]
      C -->|jsonl| Y[data/train.jsonl]
      D -->|gguf|  Z[models/finetuned.gguf]
    end
```

---

## 3Â Â·Â QuickÂ start

```bash
# clone & cd
make all OPENAI_API_KEY=sk-********
```

<details>
<summary>Manual steps</summary>

```bash
python -m venv .venv && . .venv/bin/activate
make deps
make ingest generate finetune evaluate
make demo              # interactive CLI RAG
```
</details>

---

## 4Â Â·Â Timing (ThinkPadÂ T14,Â RyzenÂ 7Â 7840U)

| Stage | Time | PeakÂ RAM | Notes |
|-------|------|----------|-------|
| Ingest (240Â pages) | 0â€¯:â€¯18â€¯m | 0.3â€¯GB | pdfplumber + splitter |
| Generate (500Â QA)  | 8â€¯:â€¯12â€¯m | 0.5â€¯GB | GPTâ€‘4oâ€‘mini |
| Fineâ€‘tune (CPU)    | 1â€¯:â€¯47â€¯h | 5â€¯GB   | 4â€‘bit QLoRA |
| Evaluate           | 0â€¯:â€¯04â€¯m | 1â€¯GB   | MiniLM retrieval |
| Demo latency       | **0.7â€¯s** | 1â€¯GB   | CPUâ€‘only RAG |

*(GPU cuts fineâ€‘tune to â‰ˆ40â€¯min.)*

---

## 5Â Â·Â Evaluation math (one paragraph)

For each question **q** in the 50â€‘item eval set we:  
1. Embed **q** with MiniLM, retrieve the topâ€‘*k*Â =Â 3 passages by cosineâ€‘sim.  
2. Mark a **Hitâ€¯@â€¯3** if the gold answer string appears in any retrieved passage.  
3. Ask each LLM to answer **q** with the passages prepended; a GPTâ€‘4oâ€‘mini judge returns **1** if the modelâ€™s answer is semantically equivalent to the reference answer, else **0**.  
Aggregating over all examples yields two accuracies: *Hitsâ€¯@â€¯3* and *LLMâ€‘judge*.  
Absolute uplift is **Î”â€¯ppÂ =Â acc<sub>tuned</sub>Â âˆ’Â acc<sub>base</sub>**; relative uplift is **Î”â€¯%Â =Â Î”â€¯ppâ€¯/â€¯acc<sub>base</sub> Ã—Â 100**.

---

## 6Â Â·Â RepoÂ map

```
src/
  ingest.py    # PDF â†’ chunks
  generate.py  # synthetic Qâ€‘A
  finetune.py  # QLoRA + GGUF
  evaluate.py  # metrics
  demo.py      # CLI RAG
tests/         # unit + CI smoke
Makefile       # help / all / smoke
config.yaml    # hyperâ€‘params & paths
.env.example   # OPENAI_API_KEY placeholder
```

---

## 7Â Â·Â DevelopmentÂ UX

| Command | Description |
|---------|-------------|
| `make help`  | task list |
| `make all`   | endâ€‘toâ€‘end pipeline |
| `make smoke` | 90â€‘s CI run (OpenAI + llama mocked) |
| `pre-commit install` | autoâ€‘format & lint on commit |
| `make clean` | remove venv, artifacts |

---

## 8Â Â·Â Licenses & attribution

* **Code** â€“ Apacheâ€‘2.0  
* **Base model** â€“ [QuantFactory/Llamaâ€‘3.2â€‘1B](https://huggingface.co/QuantFactory/Llama-3.2-1B) under the LlamaÂ 3 Community License.  
* **GPTâ€‘4oâ€‘mini** calls governed by the OpenAI Terms of Service.

---

## 9Â Â·Â FAQ

* **Do I need a GPU?** â€“ No. CPUÂ +Â 16â€¯GB RAM suffices; training just takes longer.  
* **Why MiniLM?** â€“ 384â€‘d vectors, tiny wheel, great recall for hundreds of passages.  
* **Plug in my own PDF?** â€“ Change `paths.pdf` in `config.yaml`, rerun `make ingest â€¦`.

Enjoy the fineâ€‘tuning! ðŸ¥·

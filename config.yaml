paths:
  pdf: data/raw/army_manual.pdf
  pdf_chunks_jsonl: data/pdf_chunks.jsonl
  train_jsonl:      data/train.jsonl
  eval_manual_jsonl: data/eval_manual.jsonl
  eval_holdout_jsonl: data/eval_holdout.jsonl
  finetuned_gguf: models/finetuned.gguf
  base_gguf: models/Llama-3.2-1B.Q8_0.gguf
  
ingest:
  max_tokens: 300
  overlap: 0.25

generate:
  batch_size: 25
  max_rows: 120  # bump to 500 for full run

finetune:
  base_model_id: models/llama-3.2-1b
  output_dir: models/lora_adapter
  merged_gguf: models/finetuned.gguf

  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.10
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

  batch_size: 8
  grad_accum: 4
  num_epochs: 2
  max_seq_len: 2048
  learning_rate: 2e-4

evaluate:
  top_k: 3
  max_eval: 50   
  n_threads: 8
  embed_model_path: models/minilm

demo:
  top_k: 3
  n_threads: 8

retrieval:
  top_k: 32
  rerank_final: 4
  max_ctx_tokens: 1600
